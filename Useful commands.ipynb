{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARIPRIYA02/PatchTST-AutoTuner2Muga/blob/main/Useful%20commands.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxB59a3mIDxL",
        "outputId": "d2516642-77e6-4996-98a3-3adfea6a1aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Time-Series-Library'...\n",
            "remote: Enumerating objects: 2023, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 2023 (delta 81), reused 63 (delta 63), pack-reused 1911 (from 3)\u001b[K\n",
            "Receiving objects: 100% (2023/2023), 78.28 MiB | 13.30 MiB/s, done.\n",
            "Resolving deltas: 100% (1388/1388), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/thuml/Time-Series-Library.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEecH2h5IIpN",
        "outputId": "bac70139-01eb-4517-f6ba-7b5195bc3a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Time-Series-Library\n"
          ]
        }
      ],
      "source": [
        "%cd Time-Series-Library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BZzYookILND",
        "outputId": "9228c3eb-ba2a-4961-d7b6-8a11f1515577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ULS7u7gLISMG",
        "outputId": "8934b829-6651-4057-87c4-76801e8c3fba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-65e54880-f5f9-46d7-8185-532eba9fab25\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-65e54880-f5f9-46d7-8185-532eba9fab25\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving national_newsta.csv to national_newsta.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V-yTCnfJIXqz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('./dataset/illness'):\n",
        "    os.makedirs('./dataset/illness')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oUS0Fhg-Ibjg"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, './dataset/illness')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j8E3FomIeJ4",
        "outputId": "a9113040-500a-4956-f677-8484c61fa3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/4.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyWavelets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8omuZSdrIuEz",
        "outputId": "123b4521-cbbb-4324-eff1-01b41a873391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reformer-pytorch\n",
            "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from reformer-pytorch)\n",
            "  Downloading axial_positional_embedding-0.3.12-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from reformer-pytorch) (0.8.1)\n",
            "Collecting local-attention (from reformer-pytorch)\n",
            "  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
            "Collecting product-key-memory (from reformer-pytorch)\n",
            "  Downloading product_key_memory-0.2.11-py3-none-any.whl.metadata (717 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from reformer-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->reformer-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->reformer-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->reformer-pytorch) (1.3.0)\n",
            "Collecting hyper-connections>=0.1.8 (from local-attention->reformer-pytorch)\n",
            "  Downloading hyper_connections-0.1.15-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting colt5-attention>=0.10.14 (from product-key-memory->reformer-pytorch)\n",
            "  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from colt5-attention>=0.10.14->product-key-memory->reformer-pytorch) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->reformer-pytorch) (3.0.2)\n",
            "Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
            "Downloading axial_positional_embedding-0.3.12-py3-none-any.whl (6.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading product_key_memory-0.2.11-py3-none-any.whl (6.5 kB)\n",
            "Downloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
            "Downloading hyper_connections-0.1.15-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyper-connections, axial-positional-embedding, local-attention, colt5-attention, product-key-memory, reformer-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed axial-positional-embedding-0.3.12 colt5-attention-0.11.1 hyper-connections-0.1.15 local-attention-1.11.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 product-key-memory-0.2.11 reformer-pytorch-1.4.4\n"
          ]
        }
      ],
      "source": [
        "!pip install reformer-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd6B8wiCI2rH",
        "outputId": "df776f4f-ce49-4172-f00e-750ff768024c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sktime\n",
            "  Downloading sktime-0.36.1-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: joblib<1.5,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.4.2)\n",
            "Requirement already satisfied: numpy<2.3,>=1.21 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from sktime) (24.2)\n",
            "Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.2.2)\n",
            "Collecting scikit-base<0.13.0,>=0.6.1 (from sktime)\n",
            "  Downloading scikit_base-0.12.2-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: scikit-learn<1.7.0,>=0.24 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.6.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=0.24->sktime) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.17.0)\n",
            "Downloading sktime-0.36.1-py3-none-any.whl (37.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.0/37.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_base-0.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-base, sktime\n",
            "Successfully installed scikit-base-0.12.2 sktime-0.36.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sktime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lC01qUxbQky",
        "outputId": "13a489c3-14ba-46f9-a647-09333eff5fdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat: _examples.py: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "cat _examples.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVGmu3gGI7at",
        "outputId": "df753b04-a699-4ed2-86fc-871401bc8baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-4.0.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Downloading patool-4.0.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.3/86.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install patool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-e3LnkCYelt",
        "outputId": "d6dd7f6b-9b4a-4e4c-ac7c-35aaceac2ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.10/dist-packages (2024.7.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (24.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.5.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.1.4)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
            "  Downloading dask_expr-1.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (14.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.20.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.16.0)\n",
            "Downloading dask_expr-1.1.9-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dask-expr\n",
            "Successfully installed dask-expr-1.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install dask[dataframe]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx45EYDR7nhf",
        "outputId": "cd414fd4-1044-4d01-c0e3-c380f56b9eef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "Device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cj9QKlnbC1h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/Time-Series-Library/dataset/illness/RSV_Region_10train8_sum.csv'  # Change to your file's name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df.rename(columns={'Total': 'ILITOTAL'}, inplace=True)\n",
        "df.to_csv(file_path, index=False)  # Overwrite the same file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./scripts/long_term_forecast/ILI_script/PatchTST.sh"
      ],
      "metadata": {
        "id": "b8GCZcgBFUN3",
        "outputId": "b030cf9d-3605-45c7-805b-0f3875ec115d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ili_108_12_el8_nh4_dm2048_df512Model:              PatchTST            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/illness/  \n",
            "  Data Path:          national_newsta.csv Features:           S                   \n",
            "  Target:             ILITOTAL            Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            108                 Label Len:          18                  \n",
            "  Pred Len:           12                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             7                   Dec In:             7                   \n",
            "  C Out:              7                   d model:            2048                \n",
            "  n heads:            4                   e layers:           8                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      1e-05               \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ili_108_12_el8_nh4_dm2048_df512_PatchTST_custom_ftS_sl108_ll18_pl12_dm2048_nh4_el8_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "num_train: 579\n",
            "num_test: 144\n",
            "border1s: [0, 471, 472]\n",
            "border2s: [579, 580, 724]\n",
            "self.cumulative_lengths [460]\n",
            "train 460\n",
            "num_train: 579\n",
            "num_test: 144\n",
            "border1s: [0, 471, 472]\n",
            "border2s: [579, 580, 724]\n",
            "self.cumulative_lengths [133]\n",
            "test 133\n",
            "num_train: 579\n",
            "num_test: 144\n",
            "border1s: [0, 471, 472]\n",
            "border2s: [579, 580, 724]\n",
            "self.cumulative_lengths [133]\n",
            "test 133\n",
            "Epoch: 1 cost time: 7.147301197052002\n",
            "Epoch: 1, Steps: 15 | Train Loss: 0.5605533 Vali Loss: 0.6105891 Test Loss: 0.6105891\n",
            "Validation loss decreased (inf --> 0.610589).  Saving model ...\n",
            "Updating learning rate to 1e-05\n",
            "Epoch: 2 cost time: 3.239222288131714\n",
            "Epoch: 2, Steps: 15 | Train Loss: 0.4070669 Vali Loss: 0.4588008 Test Loss: 0.4588008\n",
            "Validation loss decreased (0.610589 --> 0.458801).  Saving model ...\n",
            "Updating learning rate to 5e-06\n",
            "Epoch: 3 cost time: 3.262843132019043\n",
            "Epoch: 3, Steps: 15 | Train Loss: 0.3690016 Vali Loss: 0.4569997 Test Loss: 0.4569997\n",
            "Validation loss decreased (0.458801 --> 0.457000).  Saving model ...\n",
            "Updating learning rate to 2.5e-06\n",
            "Epoch: 4 cost time: 3.5362582206726074\n",
            "Epoch: 4, Steps: 15 | Train Loss: 0.3383144 Vali Loss: 0.4299596 Test Loss: 0.4299596\n",
            "Validation loss decreased (0.457000 --> 0.429960).  Saving model ...\n",
            "Updating learning rate to 1.25e-06\n",
            "Epoch: 5 cost time: 3.305645227432251\n",
            "Epoch: 5, Steps: 15 | Train Loss: 0.3313520 Vali Loss: 0.4270682 Test Loss: 0.4270682\n",
            "Validation loss decreased (0.429960 --> 0.427068).  Saving model ...\n",
            "Updating learning rate to 6.25e-07\n",
            "Epoch: 6 cost time: 3.3196616172790527\n",
            "Epoch: 6, Steps: 15 | Train Loss: 0.3305348 Vali Loss: 0.4287739 Test Loss: 0.4287739\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 3.125e-07\n",
            "Epoch: 7 cost time: 3.280033826828003\n",
            "Epoch: 7, Steps: 15 | Train Loss: 0.3287344 Vali Loss: 0.4297681 Test Loss: 0.4297681\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.5625e-07\n",
            "Epoch: 8 cost time: 3.1756348609924316\n",
            "Epoch: 8, Steps: 15 | Train Loss: 0.3287028 Vali Loss: 0.4280548 Test Loss: 0.4280548\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ili_108_12_el8_nh4_dm2048_df512_PatchTST_custom_ftS_sl108_ll18_pl12_dm2048_nh4_el8_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "num_train: 579\n",
            "num_test: 144\n",
            "border1s: [0, 471, 472]\n",
            "border2s: [579, 580, 724]\n",
            "self.cumulative_lengths [133]\n",
            "test 133\n",
            "test shape: (133, 12, 1) (133, 12, 1)\n",
            "test shape: (133, 12, 1) (133, 12, 1)\n",
            "(133, 12, 1)\n",
            "(1596, 1)\n",
            "(133, 12, 1)\n",
            "(133, 12, 1)\n",
            "(133, 12, 1)\n",
            "horizon:1  smape:18.95989990234375\n",
            "horizon:2  smape:20.54212760925293\n",
            "horizon:3  smape:27.965747833251953\n",
            "horizon:4  smape:32.77935028076172\n",
            "horizon:5  smape:27.580326080322266\n",
            "horizon:6  smape:33.08107376098633\n",
            "horizon:7  smape:35.09180450439453\n",
            "horizon:8  smape:35.9521484375\n",
            "horizon:9  smape:35.953392028808594\n",
            "horizon:10  smape:37.888519287109375\n",
            "horizon:11  smape:30.533781051635742\n",
            "horizon:12  smape:34.54704666137695\n",
            "horizon upto value:12, smape:30.906269073486328\n",
            "SMAPE: 30.90627\n",
            "mse:0.42706823348999023, mae:0.4476717710494995, mape:0.24737504124641418, dtw:Not calculated\n",
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ili_36_12_el6_nh4_dm2048_df512Model:              PatchTST            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/illness/  \n",
            "  Data Path:          national_newsta.csv Features:           S                   \n",
            "  Target:             ILITOTAL            Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            36                  Label Len:          18                  \n",
            "  Pred Len:           12                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             7                   Dec In:             7                   \n",
            "  C Out:              7                   d model:            2048                \n",
            "  n heads:            4                   e layers:           6                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         16                  \n",
            "  Patience:           3                   Learning Rate:      1e-05               \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ili_36_12_el6_nh4_dm2048_df512_PatchTST_custom_ftS_sl36_ll18_pl12_dm2048_nh4_el6_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "num_train: 579\n",
            "num_test: 144\n",
            "border1s: [0, 543, 544]\n",
            "border2s: [579, 580, 724]\n",
            "self.cumulative_lengths [532]\n",
            "train 532\n",
            "num_train: 579\n",
            "num_test: 144\n",
            "border1s: [0, 543, 544]\n",
            "border2s: [579, 580, 724]\n",
            "self.cumulative_lengths [133]\n",
            "test 133\n",
            "num_train: 579\n",
            "num_test: 144\n",
            "border1s: [0, 543, 544]\n",
            "border2s: [579, 580, 724]\n",
            "self.cumulative_lengths [133]\n",
            "test 133\n",
            "Epoch: 1 cost time: 7.79402756690979\n",
            "Epoch: 1, Steps: 34 | Train Loss: 0.6334072 Vali Loss: 0.4847500 Test Loss: 0.4847500\n",
            "Validation loss decreased (inf --> 0.484750).  Saving model ...\n",
            "Updating learning rate to 1e-05\n",
            "Epoch: 2 cost time: 2.526111364364624\n",
            "Epoch: 2, Steps: 34 | Train Loss: 0.3994354 Vali Loss: 0.4355454 Test Loss: 0.4355454\n",
            "Validation loss decreased (0.484750 --> 0.435545).  Saving model ...\n",
            "Updating learning rate to 5e-06\n",
            "Epoch: 3 cost time: 2.596789836883545\n",
            "Epoch: 3, Steps: 34 | Train Loss: 0.3886774 Vali Loss: 0.4303685 Test Loss: 0.4303685\n",
            "Validation loss decreased (0.435545 --> 0.430368).  Saving model ...\n",
            "Updating learning rate to 2.5e-06\n",
            "Epoch: 4 cost time: 2.991495370864868\n",
            "Epoch: 4, Steps: 34 | Train Loss: 0.3515286 Vali Loss: 0.4213759 Test Loss: 0.4213759\n",
            "Validation loss decreased (0.430368 --> 0.421376).  Saving model ...\n",
            "Updating learning rate to 1.25e-06\n",
            "Epoch: 5 cost time: 2.603532075881958\n",
            "Epoch: 5, Steps: 34 | Train Loss: 0.3384259 Vali Loss: 0.4150674 Test Loss: 0.4150674\n",
            "Validation loss decreased (0.421376 --> 0.415067).  Saving model ...\n",
            "Updating learning rate to 6.25e-07\n",
            "Epoch: 6 cost time: 2.608035087585449\n",
            "Epoch: 6, Steps: 34 | Train Loss: 0.3544216 Vali Loss: 0.4117277 Test Loss: 0.4117277\n",
            "Validation loss decreased (0.415067 --> 0.411728).  Saving model ...\n",
            "Updating learning rate to 3.125e-07\n",
            "Epoch: 7 cost time: 2.7131712436676025\n",
            "Exception ignored in: <function _releaseLock at 0x7af2e4e7dc60>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 237, in _releaseLock\n",
            "    def _releaseLock():\n",
            "    \n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-Series-Library/scripts/long_term_forecast/ILI_script/tunerpa.py\", line 152, in <module>\n",
            "    run_autotuner()\n",
            "  File \"/content/Time-Series-Library/scripts/long_term_forecast/ILI_script/tunerpa.py\", line 127, in run_autotuner\n",
            "    smape, mae, mse = train_patchtst(config)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Time-Series-Library/scripts/long_term_forecast/ILI_script/tunerpa.py\", line 98, in train_patchtst\n",
            "    subprocess.run(cmd, cwd=project_root)\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 550, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1201, in communicate\n",
            "    self.wait()\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1264, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2053, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2011, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "4kvxxXNcAdti",
        "outputId": "2ee6d4ee-d87b-4197-c780-9c18b60b06ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Time-Series-Library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "id": "PESp82nUAgU_",
        "outputId": "00f166fb-83f1-4d97-c295-ceed28b20317",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitialized existing Git repository in /content/Time-Series-Library/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n"
      ],
      "metadata": {
        "id": "BQMDA9NWAm4X"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add README.md"
      ],
      "metadata": {
        "id": "trddjPgFHQx9"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Initial commit with PatchTST autotuner and all related files\"\n"
      ],
      "metadata": {
        "id": "9zu2CwBNA7pV",
        "outputId": "e728bcbb-a7db-4162-ce5f-0250e139e7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"HARIPRIYA02\"\n",
        "!git config --global user.email \"haripriyamuppidi02@gmail.com\"\n"
      ],
      "metadata": {
        "id": "6MpxHYFLBGGN"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -M main"
      ],
      "metadata": {
        "id": "-B6c-A4_HYw6"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push https://github.com/HARIPRIYA02/PatchTST-AutoTuner2Muga.git"
      ],
      "metadata": {
        "id": "f6IIeJ43Br90"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/HARIPRIYA02/PatchTST-AutoTuner2Muga.git"
      ],
      "metadata": {
        "id": "8eGwisp6Hcwf"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote remove origin\n"
      ],
      "metadata": {
        "id": "IAx7i9e3CFh7"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/HARIPRIYA02/PatchTST-AutoTuner2Muga.git"
      ],
      "metadata": {
        "id": "wvCZxmg2EqAZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin main"
      ],
      "metadata": {
        "id": "nlEpGHK-EZ98",
        "outputId": "6c37de43-145e-4a09-cb10-a36b11e858f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!add-apt-repository ppa:git-core/ppa -y\n",
        "!apt update\n",
        "!apt install git -y\n",
        "!git --version\n"
      ],
      "metadata": {
        "id": "VNU3SeVxHx7t",
        "outputId": "f4956dcb-3614-49c8-9395-d6fc9b8b5e0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPA publishes dbgsym, you may need to include 'main/debug' component\n",
            "Repository: 'deb https://ppa.launchpadcontent.net/git-core/ppa/ubuntu/ jammy main'\n",
            "Description:\n",
            "The most current stable version of Git for Ubuntu.\n",
            "\n",
            "For release candidates, go to https://launchpad.net/~git-core/+archive/candidate .\n",
            "More info: https://launchpad.net/~git-core/+archive/ubuntu/ppa\n",
            "Adding repository.\n",
            "Adding deb entry to /etc/apt/sources.list.d/git-core-ubuntu-ppa-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/git-core-ubuntu-ppa-jammy.list\n",
            "Adding key to /etc/apt/trusted.gpg.d/git-core-ubuntu-ppa.gpg with fingerprint F911AB184317630C59970973E363C90F8F1B6217\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,383 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,810 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,099 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://ppa.launchpadcontent.net/git-core/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,684 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:18 https://ppa.launchpadcontent.net/git-core/ppa/ubuntu jammy/main amd64 Packages [4,109 B]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,000 kB]\n",
            "Fetched 26.0 MB in 3s (9,290 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/git-core/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  git-man\n",
            "Suggested packages:\n",
            "  gettext-base git-doc git-email git-gui gitk gitweb git-cvs git-mediawiki git-svn\n",
            "The following packages will be upgraded:\n",
            "  git git-man\n",
            "2 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 9,228 kB of archives.\n",
            "After this operation, 15.9 MB of additional disk space will be used.\n",
            "Get:1 https://ppa.launchpadcontent.net/git-core/ppa/ubuntu jammy/main amd64 git amd64 1:2.49.0-0ubuntu1~ubuntu22.04.1 [6,963 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/git-core/ppa/ubuntu jammy/main amd64 git-man all 1:2.49.0-0ubuntu1~ubuntu22.04.1 [2,265 kB]\n",
            "Fetched 9,228 kB in 2s (4,385 kB/s)\n",
            "(Reading database ... 126213 files and directories currently installed.)\n",
            "Preparing to unpack .../git_1%3a2.49.0-0ubuntu1~ubuntu22.04.1_amd64.deb ...\n",
            "Unpacking git (1:2.49.0-0ubuntu1~ubuntu22.04.1) over (1:2.34.1-1ubuntu1.12) ...\n",
            "Preparing to unpack .../git-man_1%3a2.49.0-0ubuntu1~ubuntu22.04.1_all.deb ...\n",
            "Unpacking git-man (1:2.49.0-0ubuntu1~ubuntu22.04.1) over (1:2.34.1-1ubuntu1.12) ...\n",
            "Setting up git-man (1:2.49.0-0ubuntu1~ubuntu22.04.1) ...\n",
            "Setting up git (1:2.49.0-0ubuntu1~ubuntu22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "git version 2.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t  -C \"haripriyamuppidi02@gmail.com\" -f ~/.ssh/id_ -N \"\" generate key in next step\n"
      ],
      "metadata": {
        "id": "aOCQUM30Ii0R",
        "outputId": "0610f907-c01e-4bb5-cf05-0a18cac4081b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private ed25519 key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_ed25519): ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push -u origin main\n"
      ],
      "metadata": {
        "id": "EFKcSBu-J4O0",
        "outputId": "70c1f3e5-2079-480c-c536-7d0652fd22c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 1987, done.\n",
            "Counting objects:   0% (1/1987)\rCounting objects:   1% (20/1987)\rCounting objects:   2% (40/1987)\rCounting objects:   3% (60/1987)\rCounting objects:   4% (80/1987)\rCounting objects:   5% (100/1987)\rCounting objects:   6% (120/1987)\rCounting objects:   7% (140/1987)\rCounting objects:   8% (159/1987)\rCounting objects:   9% (179/1987)\rCounting objects:  10% (199/1987)\rCounting objects:  11% (219/1987)\rCounting objects:  12% (239/1987)\rCounting objects:  13% (259/1987)\rCounting objects:  14% (279/1987)\rCounting objects:  15% (299/1987)\rCounting objects:  16% (318/1987)\rCounting objects:  17% (338/1987)\rCounting objects:  18% (358/1987)\rCounting objects:  19% (378/1987)\rCounting objects:  20% (398/1987)\rCounting objects:  21% (418/1987)\rCounting objects:  22% (438/1987)\rCounting objects:  23% (458/1987)\rCounting objects:  24% (477/1987)\rCounting objects:  25% (497/1987)\rCounting objects:  26% (517/1987)\rCounting objects:  27% (537/1987)\rCounting objects:  28% (557/1987)\rCounting objects:  29% (577/1987)\rCounting objects:  30% (597/1987)\rCounting objects:  31% (616/1987)\rCounting objects:  32% (636/1987)\rCounting objects:  33% (656/1987)\rCounting objects:  34% (676/1987)\rCounting objects:  35% (696/1987)\rCounting objects:  36% (716/1987)\rCounting objects:  37% (736/1987)\rCounting objects:  38% (756/1987)\rCounting objects:  39% (775/1987)\rCounting objects:  40% (795/1987)\rCounting objects:  41% (815/1987)\rCounting objects:  42% (835/1987)\rCounting objects:  43% (855/1987)\rCounting objects:  44% (875/1987)\rCounting objects:  45% (895/1987)\rCounting objects:  46% (915/1987)\rCounting objects:  47% (934/1987)\rCounting objects:  48% (954/1987)\rCounting objects:  49% (974/1987)\rCounting objects:  50% (994/1987)\rCounting objects:  51% (1014/1987)\rCounting objects:  52% (1034/1987)\rCounting objects:  53% (1054/1987)\rCounting objects:  54% (1073/1987)\rCounting objects:  55% (1093/1987)\rCounting objects:  56% (1113/1987)\rCounting objects:  57% (1133/1987)\rCounting objects:  58% (1153/1987)\rCounting objects:  59% (1173/1987)\rCounting objects:  60% (1193/1987)\rCounting objects:  61% (1213/1987)\rCounting objects:  62% (1232/1987)\rCounting objects:  63% (1252/1987)\rCounting objects:  64% (1272/1987)\rCounting objects:  65% (1292/1987)\rCounting objects:  66% (1312/1987)\rCounting objects:  67% (1332/1987)\rCounting objects:  68% (1352/1987)\rCounting objects:  69% (1372/1987)\rCounting objects:  70% (1391/1987)\rCounting objects:  71% (1411/1987)\rCounting objects:  72% (1431/1987)\rCounting objects:  73% (1451/1987)\rCounting objects:  74% (1471/1987)\rCounting objects:  75% (1491/1987)\rCounting objects:  76% (1511/1987)\rCounting objects:  77% (1530/1987)\rCounting objects:  78% (1550/1987)\rCounting objects:  79% (1570/1987)\rCounting objects:  80% (1590/1987)\rCounting objects:  81% (1610/1987)\rCounting objects:  82% (1630/1987)\rCounting objects:  83% (1650/1987)\rCounting objects:  84% (1670/1987)\rCounting objects:  85% (1689/1987)\rCounting objects:  86% (1709/1987)\rCounting objects:  87% (1729/1987)\rCounting objects:  88% (1749/1987)\rCounting objects:  89% (1769/1987)\rCounting objects:  90% (1789/1987)\rCounting objects:  91% (1809/1987)\rCounting objects:  92% (1829/1987)\rCounting objects:  93% (1848/1987)\rCounting objects:  94% (1868/1987)\rCounting objects:  95% (1888/1987)\rCounting objects:  96% (1908/1987)\rCounting objects:  97% (1928/1987)\rCounting objects:  98% (1948/1987)\rCounting objects:  99% (1968/1987)\rCounting objects: 100% (1987/1987)\rCounting objects: 100% (1987/1987), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (599/599), done.\n",
            "Writing objects: 100% (1987/1987), 78.26 MiB | 15.02 MiB/s, done.\n",
            "Total 1987 (delta 1376), reused 1957 (delta 1358), pack-reused 0 (from 0)\n",
            "remote: Resolving deltas: 100% (1376/1376), done.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: File checkpoints/long_term_forecast_ETTh1_96_96_Transformer_ETTh1_ftMS_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth is 59.76 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To github.com:HARIPRIYA02/PatchTST-AutoTuner2Muga.git\n",
            " * [new branch]      main -> main\n",
            "branch 'main' set up to track 'origin/main'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keyscan github.com >> ~/.ssh/known_hosts\n"
      ],
      "metadata": {
        "id": "DQ4AeaOwKB_w",
        "outputId": "205e30d8-5725-47a0-a8e4-41425066d45b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-91b3cc9c\n",
            "# github.com:22 SSH-2.0-91b3cc9c\n",
            "# github.com:22 SSH-2.0-91b3cc9c\n",
            "# github.com:22 SSH-2.0-91b3cc9c\n",
            "# github.com:22 SSH-2.0-91b3cc9c\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}